import torch
import numpy as np
from PIL import Image, ImageColor
import cv2
import math
import time
import re
from collections import defaultdict


class ImageBatchExtract:
    """
    æ‰¹é‡å›¾åƒæå–èŠ‚ç‚¹
    æ”¯æŒå¤šç§æå–æ¨¡å¼ï¼šè‡ªå®šä¹‰ç´¢å¼•ã€æ­¥é•¿é—´éš”ã€æ€»å¸§æ•°è‡ªåŠ¨è®¡ç®—
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "mode": (["index", "step", "uniform"], {"default": "step"}),
                "index": ("STRING", {"default": "0"}),
                "step": ("INT", {"default": 4, "min": 1, "max": 8192, "step": 1}),
                "uniform": ("INT", {"default": 4, "min": 0, "max": 8192, "step": 1}),
                "max_keep": ("INT", {"default": 10, "min": 0, "max": 8192, "step": 1}),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "extract_batch"
    CATEGORY = "1hewNodes/batch"
    
    def extract_batch(self, image, mode, index="", step=1, uniform=0, max_keep=1024):
        try:
            batch_size = image.shape[0]
            print(f"[ImageBatchExtract] è¾“å…¥æ‰¹é‡å›¾åƒä¿¡æ¯: å½¢çŠ¶={image.shape}, æ€»å¸§æ•°={batch_size}")
            print(f"[ImageBatchExtract] æå–å‚æ•°: æ¨¡å¼={mode}, ç´¢å¼•='{index}', æ­¥é•¿={step}, æ•°é‡={uniform}, æœ€å¤§ä¿ç•™={max_keep}")
            
            # æ ¹æ®æ¨¡å¼ç¡®å®šæå–ç´¢å¼•
            extract_indices = self._get_extract_indices(batch_size, mode, index, step, uniform)
            
            if not extract_indices:
                print(f"[ImageBatchExtract] æ²¡æœ‰æœ‰æ•ˆçš„æå–ç´¢å¼•ï¼Œè¿”å›ç©ºç»“æœ")
                empty_image = torch.empty((0,) + image.shape[1:], 
                                        dtype=image.dtype, device=image.device)
                return (empty_image,)
            
            # æå–å›¾åƒ
            extracted_images = []
            valid_indices = []
            
            for idx in extract_indices:
                if 0 <= idx < batch_size:
                    extracted_images.append(image[idx:idx+1])
                    valid_indices.append(idx)
                else:
                    print(f"[ImageBatchExtract] è·³è¿‡è¶…å‡ºèŒƒå›´çš„ç´¢å¼•: {idx} (æ€»å¸§æ•°: {batch_size})")
            
            if not extracted_images:
                print(f"[ImageBatchExtract] æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºèŒƒå›´ï¼Œè¿”å›ç©ºç»“æœ")
                empty_image = torch.empty((0,) + image.shape[1:], 
                                        dtype=image.dtype, device=image.device)
                return (empty_image,)
            
            # åº”ç”¨æœ€å¤§ä¿ç•™é™åˆ¶ï¼ˆmax_keep=0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            if max_keep > 0 and len(extracted_images) > max_keep:
                print(f"[ImageBatchExtract] åº”ç”¨æœ€å¤§ä¿ç•™é™åˆ¶: {len(extracted_images)} -> {max_keep}")
                extracted_images = extracted_images[:max_keep]
                valid_indices = valid_indices[:max_keep]
            elif max_keep == 0:
                print(f"[ImageBatchExtract] max_keep=0ï¼Œä¸é™åˆ¶æœ€å¤§ä¿ç•™æ•°é‡ï¼Œä¿ç•™æ‰€æœ‰{len(extracted_images)}å¼ å›¾åƒ")
            
            # åˆå¹¶æå–çš„å›¾åƒ
            result_images = torch.cat(extracted_images, dim=0)
            source_indices_str = ",".join(map(str, valid_indices))
            
            print(f"[ImageBatchExtract] æå–å®Œæˆ: æå–äº†{len(valid_indices)}å¼ å›¾åƒï¼Œç´¢å¼•=[{source_indices_str}]")
            print(f"[ImageBatchExtract] è¾“å‡ºå½¢çŠ¶: {result_images.shape}")
            
            return (result_images,)
            
        except Exception as e:
            print(f"[ImageBatchExtract] é”™è¯¯: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›ç©ºç»“æœ
            empty_image = torch.empty((0,) + image.shape[1:], 
                                    dtype=image.dtype, device=image.device)
            return (empty_image,)
    
    def _get_extract_indices(self, batch_size, mode, index, step, uniform):
        """æ ¹æ®æå–æ¨¡å¼è·å–ç´¢å¼•åˆ—è¡¨"""
        extract_indices = []
        
        try:
            if mode == "index":
                 # è‡ªå®šä¹‰ç´¢å¼•æ¨¡å¼ï¼šä¸ºç©ºå°±è¾“å‡ºç©º
                 if not index.strip():
                     print(f"[ImageBatchExtract] è‡ªå®šä¹‰ç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
                     return []
                 print(f"[ImageBatchExtract] ä½¿ç”¨è‡ªå®šä¹‰ç´¢å¼•æ¨¡å¼: '{index}'")
                 extract_indices = self._parse_custom_indices(index, batch_size)
                
            elif mode == "step":
                # æ­¥é•¿æ¨¡å¼ï¼šstepä»1å¼€å§‹
                if step < 1:
                    print(f"[ImageBatchExtract] æ­¥é•¿å°äº1ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                print(f"[ImageBatchExtract] ä½¿ç”¨æ­¥é•¿æ¨¡å¼: æ­¥é•¿{step}")
                extract_indices = self._calculate_step_indices(batch_size, step)
                
            elif mode == "uniform":
                # æ•°é‡æ¨¡å¼ï¼šuniformä¸º0è¾“å‡ºç©ºï¼Œ1é¦–å¸§ï¼Œ2é¦–å°¾å¸§ï¼Œä¾æ¬¡ç±»æ¨
                if uniform <= 0:
                    print(f"[ImageBatchExtract] æ•°é‡ä¸º0æˆ–è´Ÿæ•°ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                print(f"[ImageBatchExtract] ä½¿ç”¨æ•°é‡æ¨¡å¼: æ•°é‡{uniform}")
                extract_indices = self._calculate_count_indices(batch_size, uniform)
            
            print(f"[ImageBatchExtract] è®¡ç®—å¾—åˆ°ç´¢å¼•: {extract_indices}")
            return extract_indices
            
        except Exception as e:
            print(f"[ImageBatchExtract] ç´¢å¼•è®¡ç®—é”™è¯¯: {str(e)}")
            return []
    
    def _parse_custom_indices(self, indices_str, batch_size=None):
        """
        è§£æè‡ªå®šä¹‰ç´¢å¼•å­—ç¬¦ä¸²ï¼Œæ”¯æŒè´Ÿæ•°ç´¢å¼•
        æ”¯æŒæ ¼å¼: "1,3,5,20" æˆ– "1, 3, 5, 20" æˆ– "-1,-2,0" æˆ– "1ï¼Œ2ï¼Œ-1"ï¼ˆä¸­æ–‡é€—å·ï¼‰
        ä¿æŒè¾“å…¥é¡ºåºï¼Œæ”¯æŒä¸­è‹±æ–‡é€—å·åˆ†å‰²ï¼Œå¤„ç†ç©ºæ ¼å’Œç©ºå†…å®¹
        """
        indices = []
        try:
            # æ›¿æ¢ä¸­æ–‡é€—å·ä¸ºè‹±æ–‡é€—å·ï¼Œç„¶ååˆ†å‰²
            normalized_str = indices_str.replace('ï¼Œ', ',')
            parts = normalized_str.split(',')
            
            for part in parts:
                # å»é™¤ç©ºæ ¼
                part = part.strip()
                # è·³è¿‡ç©ºå†…å®¹
                if not part:
                    continue
                    
                try:
                    idx = int(part)
                    # å¤„ç†è´Ÿæ•°ç´¢å¼•
                    if batch_size is not None and idx < 0:
                        idx = batch_size + idx
                    indices.append(idx)
                except ValueError:
                    print(f"[ImageBatchExtract] è·³è¿‡æ— æ•ˆç´¢å¼•: '{part}'")
                    continue
            
            print(f"[ImageBatchExtract] è§£æè‡ªå®šä¹‰ç´¢å¼•: '{indices_str}' -> {indices}")
            
        except Exception as e:
            print(f"[ImageBatchExtract] è‡ªå®šä¹‰ç´¢å¼•è§£æé”™è¯¯: {str(e)}")
            indices = []
        
        return indices
    
    def _calculate_step_indices(self, batch_size, step):
        """è®¡ç®—æ­¥é•¿ç´¢å¼•ï¼Œä»0å¼€å§‹ï¼Œæ­¥é•¿ä»1å¼€å§‹"""
        indices = list(range(0, batch_size, step))
        print(f"[ImageBatchExtract] æ­¥é•¿è®¡ç®—: æ€»å¸§æ•°={batch_size}, æ­¥é•¿={step} -> {indices}")
        return indices
    
    def _calculate_count_indices(self, batch_size, count):
        """
        æ ¹æ®æ•°é‡è®¡ç®—ç´¢å¼•
        count=1: é¦–å¸§ [0]
        count=2: é¦–å°¾å¸§ [0, batch_size-1]
        count=3: é¦–ä¸­å°¾å¸§ [0, middle, batch_size-1]
        ä¾æ¬¡ç±»æ¨
        """
        if count <= 0:
            return []
        
        if count == 1:
            # åªè¦é¦–å¸§
            indices = [0]
        elif count == 2:
            # é¦–å°¾å¸§
            indices = [0, batch_size - 1] if batch_size > 1 else [0]
        elif count >= batch_size:
            # æ•°é‡å¤§äºç­‰äºæ€»å¸§æ•°ï¼Œè¿”å›æ‰€æœ‰å¸§
            indices = list(range(batch_size))
        else:
            # å‡åŒ€åˆ†å¸ƒ
            step = (batch_size - 1) / (count - 1)
            indices = [int(round(i * step)) for i in range(count)]
            # ç¡®ä¿æœ€åä¸€å¸§æ˜¯æœ€åä¸€ä¸ªç´¢å¼•
            indices[-1] = batch_size - 1
            # å»é‡å¹¶æ’åº
            indices = sorted(list(set(indices)))
        
        print(f"[ImageBatchExtract] æ•°é‡è®¡ç®—: æ€»å¸§æ•°={batch_size}, æ•°é‡={count} -> {indices}")
        return indices
  

class ImageBatchSplit:
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "take_count": ("INT", {"default": 8, "min": 1, "max": 1024, "step": 1}),
                "from_start": ("BOOLEAN", {"default": False}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("image_1", "image_2")
    FUNCTION = "split_batch"
    CATEGORY = "1hewNodes/batch"
    
    def split_batch(self, image, take_count, from_start=False):
        try:
            # è·å–æ‰¹æ¬¡å¤§å°
            batch_size = image.shape[0]
            print(f"[ImageBatchSplit] è¾“å…¥å›¾ç‰‡æ‰¹æ¬¡ä¿¡æ¯: å½¢çŠ¶={image.shape}, æ•°æ®ç±»å‹={image.dtype}, è®¾å¤‡={image.device}")
            print(f"[ImageBatchSplit] æ‹†åˆ†å‚æ•°: æ€»å›¾ç‰‡æ•°={batch_size}, å–æ•°={take_count}, ä»å¼€å¤´åˆ‡={from_start}")
            
            # éªŒè¯æ‹†åˆ†æ•°é‡
            if take_count >= batch_size:
                print(f"[ImageBatchSplit] è¾¹ç•Œæƒ…å†µ: å–æ•°({take_count})å¤§äºç­‰äºæ€»å›¾ç‰‡æ•°({batch_size})")
                
                if from_start:
                    # ä»å¼€å¤´åˆ‡ï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯å…¨éƒ¨å›¾ç‰‡ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸ºç©º
                    print(f"[ImageBatchSplit] from_start=True: ç¬¬ä¸€éƒ¨åˆ†=å…¨éƒ¨å›¾ç‰‡ï¼Œç¬¬äºŒéƒ¨åˆ†=ç©º")
                    empty_second = torch.empty((0,) + image.shape[1:], dtype=image.dtype, device=image.device)
                    print(f"[ImageBatchSplit] è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=åŸå›¾ç‰‡({batch_size}å¼ ), ç¬¬äºŒéƒ¨åˆ†=ç©ºå¼ é‡")
                    return (image, empty_second)
                else:
                    # ä»ç»“å°¾åˆ‡ï¼šç¬¬ä¸€éƒ¨åˆ†ä¸ºç©ºï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯å…¨éƒ¨å›¾ç‰‡
                    print(f"[ImageBatchSplit] from_start=False: ç¬¬ä¸€éƒ¨åˆ†=ç©ºï¼Œç¬¬äºŒéƒ¨åˆ†=å…¨éƒ¨å›¾ç‰‡")
                    empty_first = torch.empty((0,) + image.shape[1:], dtype=image.dtype, device=image.device)
                    print(f"[ImageBatchSplit] è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=ç©ºå¼ é‡, ç¬¬äºŒéƒ¨åˆ†=åŸå›¾ç‰‡({batch_size}å¼ )")
                    return (empty_first, image)
            
            # æ ¹æ®å‚æ•°è®¡ç®—æ‹†åˆ†ä½ç½®
            if from_start:
                # ä»å¼€å¤´åˆ‡ï¼šsplit_countæ˜¯ç¬¬ä¸€éƒ¨åˆ†çš„æ•°é‡
                first_count = take_count
                second_count = batch_size - take_count
                first_batch = image[:first_count]
                second_batch = image[first_count:]
                print(f"[ImageBatchSplit] from_start=Trueæ‹†åˆ†å®Œæˆ: æ€»æ•°{batch_size} -> ç¬¬ä¸€éƒ¨åˆ†{first_count}å¼ , ç¬¬äºŒéƒ¨åˆ†{second_count}å¼ ")
            else:
                # ä»ç»“å°¾åˆ‡ï¼šsplit_countæ˜¯ç¬¬äºŒéƒ¨åˆ†çš„æ•°é‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                first_count = batch_size - take_count
                second_count = take_count
                first_batch = image[:first_count]
                second_batch = image[first_count:]
                print(f"[ImageBatchSplit] from_start=Falseæ‹†åˆ†å®Œæˆ: æ€»æ•°{batch_size} -> ç¬¬ä¸€éƒ¨åˆ†{first_count}å¼ , ç¬¬äºŒéƒ¨åˆ†{second_count}å¼ ")
            
            print(f"[ImageBatchSplit] è¾“å‡ºå½¢çŠ¶: ç¬¬ä¸€éƒ¨åˆ†={first_batch.shape}, ç¬¬äºŒéƒ¨åˆ†={second_batch.shape}")
            return (first_batch, second_batch)
            
        except Exception as e:
            print(f"[ImageBatchSplit] é”™è¯¯: {str(e)}")
            print(f"[ImageBatchSplit] å¼‚å¸¸å¤„ç†: è¿”å›åŸå›¾ç‰‡å’Œç©ºå¼ é‡")
            # å‡ºé”™æ—¶è¿”å›åŸå›¾ç‰‡å’Œç©ºå¼ é‡
            empty_batch = torch.empty((0,) + image.shape[1:], dtype=image.dtype, device=image.device)
            print(f"[ImageBatchSplit] å¼‚å¸¸è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=åŸå›¾ç‰‡, ç¬¬äºŒéƒ¨åˆ†=ç©ºå¼ é‡")
            return (image, empty_batch)


class ImageBatchGroup:
    """
    å›¾åƒæ‰¹æ¬¡åˆ†ç»„å™¨ - å°†æ‰¹é‡å›¾ç‰‡æŒ‰æŒ‡å®šå¤§å°åˆ†ç»„å¤„ç†
    æ”¯æŒé‡å å¸§å’Œå¤šç§æœ€åä¸€ç»„å¤„ç†æ–¹å¼
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "batch_size": ("INT", {"default": 81, "min": 1, "max": 1024, "step": 4}),
                "overlap": ("INT", {"default": 0, "min": 0, "max": 1024, "step": 1}),
                "last_batch_mode": (["drop_incomplete", "keep_remaining", "backtrack_last", "fill_color"], {"default": "backtrack_last"})
            },
            "optional": {
                "color": ("STRING", {"default": "1.0"}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "INT", "INT", "INT", "INT")
    RETURN_NAMES = ("image", "group_total", "start_index", "batch_count", "valid_count")
    OUTPUT_IS_LIST = (False, False, True, True, True)
    CATEGORY = "1hewNodes/batch"
    FUNCTION = "split_batch_sequential"
    
    def parse_color(self, color_str):
        """è§£æä¸åŒæ ¼å¼çš„é¢œè‰²è¾“å…¥ï¼Œæ”¯æŒå¤šç§é¢œè‰²æ ¼å¼"""
        if not color_str:
            return (0, 0, 0)
        
        # ç§»é™¤æ‹¬å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        color_str = color_str.strip()
        if color_str.startswith('(') and color_str.endswith(')'):
            color_str = color_str[1:-1].strip()
        
        # æ”¯æŒå•å­—æ¯é¢œè‰²ç¼©å†™
        color_shortcuts = {
            'r': 'red', 'g': 'green', 'b': 'blue', 'c': 'cyan', 
            'm': 'magenta', 'y': 'yellow', 'k': 'black', 'w': 'white'
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå•å­—æ¯ç¼©å†™
        if len(color_str) == 1 and color_str.lower() in color_shortcuts:
            color_str = color_shortcuts[color_str.lower()]
        
        # å°è¯•è§£æä¸ºç°åº¦å€¼ (0.0-1.0)
        try:
            gray = float(color_str)
            if 0.0 <= gray <= 1.0:
                return (int(gray * 255), int(gray * 255), int(gray * 255))
        except ValueError:
            pass
        
        # å°è¯•è§£æä¸º RGB æ ¼å¼ (å¦‚ "0.5,0.7,0.9" æˆ– "128,192,255")
        if ',' in color_str:
            try:
                # åˆ†å‰²å¹¶æ¸…ç†æ¯ä¸ªéƒ¨åˆ†
                parts = [part.strip() for part in color_str.split(',')]
                if len(parts) >= 3:
                    r, g, b = [float(parts[i]) for i in range(3)]
                    # åˆ¤æ–­æ˜¯å¦ä¸º 0-1 èŒƒå›´
                    if max(r, g, b) <= 1.0:
                        return (int(r * 255), int(g * 255), int(b * 255))
                    else:
                        return (int(r), int(g), int(b))
            except (ValueError, IndexError):
                pass
        
        # å°è¯•è§£æä¸ºåå…­è¿›åˆ¶æˆ–é¢œè‰²åç§°
        try:
            return ImageColor.getrgb(color_str)
        except ValueError:
            # é»˜è®¤è¿”å›ç™½è‰²
            return (255, 255, 255)
    
    def _create_white_image(self, reference_image, color_str="1.0"):
        """åˆ›å»ºä¸å‚è€ƒå›¾åƒç›¸åŒå°ºå¯¸çš„æŒ‡å®šé¢œè‰²å›¾åƒ"""
        # è§£æé¢œè‰²
        rgb_color = self.parse_color(color_str)
        r = rgb_color[0] / 255.0
        g = rgb_color[1] / 255.0
        b = rgb_color[2] / 255.0
        
        # ç¡®ä¿åˆ›å»ºçš„å›¾åƒä¸è¾“å…¥å›¾åƒå…·æœ‰ç›¸åŒçš„ç»´åº¦
        if len(reference_image.shape) == 4:
            # å¦‚æœè¾“å…¥æ˜¯4ç»´ (batch, height, width, channels)ï¼Œå–ç¬¬ä¸€ä¸ªå›¾åƒ
            height, width, channels = reference_image[0].shape
            colored_image = torch.ones((1, height, width, channels), 
                                     dtype=reference_image.dtype, 
                                     device=reference_image.device)
        else:
            # å¦‚æœè¾“å…¥æ˜¯3ç»´ (height, width, channels)
            height, width, channels = reference_image.shape
            colored_image = torch.ones((1, height, width, channels), 
                                     dtype=reference_image.dtype, 
                                     device=reference_image.device)
        
        # æ ¹æ®é€šé“æ•°è®¾ç½®é¢œè‰²
        if channels == 1:
            # ç°åº¦å›¾åƒï¼Œä½¿ç”¨ RGB çš„å¹³å‡å€¼ä½œä¸ºç°åº¦å€¼
            gray_value = (r + g + b) / 3.0
            colored_image[0, :, :, 0] = gray_value
        elif channels >= 3:
            # RGB æˆ– RGBA å›¾åƒ
            colored_image[0, :, :, 0] = r
            colored_image[0, :, :, 1] = g
            colored_image[0, :, :, 2] = b
            # å¦‚æœæ˜¯ RGBAï¼Œè®¾ç½® alpha é€šé“ä¸ºå®Œå…¨ä¸é€æ˜
            if channels == 4:
                colored_image[0, :, :, 3] = 1.0
        
        return colored_image
    
    def _validate_parameters(self, total_images, batch_size, overlap, last_batch_mode=None):
        """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
        if total_images < 1:
            raise ValueError("è¾“å…¥å›¾ç‰‡æ•°é‡å¿…é¡»å¤§äº0")
        
        if batch_size < 1:
            raise ValueError("æ‰¹æ¬¡å¤§å°å¿…é¡»å¤§äº0")
        
        if overlap < 0:
            raise ValueError("é‡å å¸§æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        # åœ¨ backtrack_last æ¨¡å¼ä¸‹ï¼Œå…è®¸ overlap ç­‰äº batch_size
        if last_batch_mode == "backtrack_last":
            if overlap > batch_size:
                raise ValueError(f"é‡å å¸§æ•° ({overlap}) ä¸èƒ½å¤§äºæ‰¹æ¬¡å¤§å° ({batch_size})")
        else:
            if overlap >= batch_size:
                raise ValueError(f"é‡å å¸§æ•° ({overlap}) å¿…é¡»å°äºæ‰¹æ¬¡å¤§å° ({batch_size})")
    
    def _calculate_start_indices(self, total_images, batch_size, overlap, last_batch_mode):
        """ç»Ÿä¸€è®¡ç®—æ‰€æœ‰æ‰¹æ¬¡çš„èµ·å§‹ç´¢å¼•"""
        if total_images <= batch_size:
            # è¾¹ç•Œæƒ…å†µï¼šå½“è¾“å…¥å¼ æ•° <= batch_size æ—¶çš„ç‰¹æ®Šå¤„ç†
            if last_batch_mode == "drop_incomplete":
                # drop_incomplete æ¨¡å¼ï¼šå¦‚æœå›¾åƒæ•°é‡ä¸è¶³ä¸€ä¸ªå®Œæ•´æ‰¹æ¬¡ï¼Œè¿”å›ç©ºåˆ—è¡¨
                return []
            else:
                # keep_remaining, backtrack_last, fill_color æ¨¡å¼ï¼šéƒ½ä»ç´¢å¼•0å¼€å§‹
                return [0]
        
        # è®¡ç®—åŸºç¡€æ­¥é•¿
        step_size = batch_size - overlap
        if step_size <= 0:
            # å½“ overlap >= batch_size æ—¶çš„ç‰¹æ®Šå¤„ç†
            if overlap == batch_size:
                step_size = max(1, (batch_size + 1) // 2)
            else:
                step_size = 1
        
        # ç”Ÿæˆæ‰¹æ¬¡èµ·å§‹ä½ç½®
        start_indices = []
        current_start = 0
        
        while current_start < total_images:
            # å¯¹äºdrop_incompleteæ¨¡å¼ï¼Œæ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦å®Œæ•´
            if last_batch_mode == "drop_incomplete":
                # å¦‚æœå½“å‰æ‰¹æ¬¡ä¸èƒ½æ»¡è¶³å®Œæ•´çš„batch_sizeï¼Œåˆ™ç»ˆæ­¢
                if current_start + batch_size > total_images:
                    break
            
            start_indices.append(current_start)
            current_start += step_size
            
            # å¯¹äºébacktrack_lastå’Œédrop_incompleteæ¨¡å¼ï¼Œå¦‚æœå½“å‰æ‰¹æ¬¡å·²ç»èƒ½è¦†ç›–åˆ°æœ€åä¸€ä¸ªå›¾ç‰‡ï¼Œåˆ™æ— éœ€ç»§ç»­
            if (last_batch_mode not in ["backtrack_last", "drop_incomplete"] and 
                len(start_indices) > 0 and 
                start_indices[-1] + batch_size >= total_images):
                break
        
        # æ ¹æ®æ¨¡å¼è°ƒæ•´æœ€åä¸€æ‰¹çš„ä½ç½®
        if last_batch_mode == "backtrack_last" and len(start_indices) > 1:
            # æœ€åä¸€æ‰¹ä»æœ«å°¾å¼€å§‹
            last_start = total_images - batch_size
            
            # ç¡®ä¿æœ€åä¸€æ‰¹ä¸ä¼šä¸ç¬¬ä¸€æ‰¹é‡å ï¼ˆç¬¬ä¸€æ‰¹å¿…é¡»ä»0å¼€å§‹ï¼‰
            if last_start <= 0:
                # å¦‚æœåªéœ€è¦ä¸€æ‰¹å°±èƒ½è¦†ç›–æ‰€æœ‰å›¾åƒï¼Œä¿æŒç¬¬ä¸€æ‰¹ä»0å¼€å§‹
                start_indices = [0]
            else:
                # è°ƒæ•´æœ€åä¸€æ‰¹ä½ç½®ï¼Œä½†ä¿æŒä¸­é—´æ‰¹æ¬¡
                # æ£€æŸ¥æœ€åä¸€æ‰¹æ˜¯å¦ä¸ç°æœ‰æ‰¹æ¬¡é‡å è¿‡å¤š
                if last_start < start_indices[-1]:
                    # å¦‚æœæœ€åä¸€æ‰¹ä½ç½®å‘å‰ç§»åŠ¨ï¼Œéœ€è¦è°ƒæ•´åºåˆ—
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¼šä¸last_starté‡å çš„æ‰¹æ¬¡
                    valid_indices = [0]  # ç¬¬ä¸€æ‰¹æ€»æ˜¯ä»0å¼€å§‹
                    
                    for i in range(1, len(start_indices)):
                        # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¸last_startæ‰¹æ¬¡é‡å è¿‡å¤š
                        current_end = start_indices[i] + batch_size - 1
                        last_start_end = last_start + batch_size - 1
                        
                        # å¦‚æœå½“å‰æ‰¹æ¬¡çš„ç»“æŸä½ç½® + overlap < last_startï¼Œåˆ™ä¿ç•™
                        if start_indices[i] + overlap <= last_start:
                            valid_indices.append(start_indices[i])
                    
                    # æ·»åŠ æœ€åä¸€æ‰¹
                    if valid_indices[-1] != last_start:
                        valid_indices.append(last_start)
                    
                    start_indices = valid_indices
                else:
                    # æœ€åä¸€æ‰¹ä½ç½®åˆç†ï¼Œç›´æ¥è°ƒæ•´
                    start_indices[-1] = last_start
        
        return start_indices
    
    def _calculate_batch_counts(self, start_indices, total_images, batch_size, last_batch_mode):
        """æ ¹æ®èµ·å§‹ç´¢å¼•å’Œæ¨¡å¼è®¡ç®—æ¯æ‰¹æ¬¡çš„æ•°é‡"""
        batch_counts = []
        
        # è¾¹ç•Œæƒ…å†µï¼šå½“è¾“å…¥å›¾åƒæ•°é‡ <= batch_size æ—¶çš„ç‰¹æ®Šå¤„ç†
        if total_images <= batch_size:
            if len(start_indices) == 0:
                # drop_incomplete æ¨¡å¼è¿”å›ç©ºåˆ—è¡¨
                return []
            elif last_batch_mode == "fill_color":
                # fill_color æ¨¡å¼ï¼šbatch_count ä½¿ç”¨ batch_size
                return [batch_size]
            else:
                # keep_remaining, backtrack_last æ¨¡å¼ï¼šbatch_count ä½¿ç”¨å®é™…å›¾åƒæ•°é‡
                return [total_images]
        
        for i, start_idx in enumerate(start_indices):
            remaining = total_images - start_idx
            
            if i == len(start_indices) - 1:
                # æœ€åä¸€æ‰¹
                if last_batch_mode == "fill_color":
                    # è¡¥å……å½©è‰²å›¾æ¨¡å¼ï¼šæ€»æ˜¯ä¿æŒæ‰¹æ¬¡å¤§å°
                    batch_counts.append(batch_size)
                elif last_batch_mode == "drop_incomplete":
                    # drop_incomplete æ¨¡å¼ï¼šä¿ç•™çš„æ‰¹æ¬¡éƒ½æ˜¯å®Œæ•´çš„
                    batch_counts.append(batch_size)
                elif last_batch_mode == "backtrack_last":
                    # backtrack_last æ¨¡å¼ï¼š
                    if len(start_indices) == 1:
                        # å•æ‰¹æ¬¡ï¼šä½¿ç”¨å‰©ä½™æ•°é‡ï¼Œä½†ä¸è¶…è¿‡total_images
                        batch_count = min(remaining, total_images)
                        batch_counts.append(batch_count)
                    else:
                        # å¤šæ‰¹æ¬¡ï¼šä¿æŒæ‰¹æ¬¡å¤§å°
                        batch_counts.append(batch_size)
                else:
                    # keep_remaining æ¨¡å¼ï¼šä½¿ç”¨å®é™…å‰©ä½™æ•°é‡ï¼Œä½†ä¸è¶…è¿‡total_images
                    batch_count = min(remaining, total_images)
                    batch_counts.append(batch_count)
            else:
                # éæœ€åä¸€æ‰¹ï¼šæ€»æ˜¯ä½¿ç”¨æ‰¹æ¬¡å¤§å°
                batch_counts.append(batch_size)
        
        return batch_counts
    
    def _calculate_valid_counts(self, start_indices, batch_counts, overlap, last_batch_mode, total_images=None):
        """è®¡ç®—æ¯æ‰¹æ¬¡çš„æœ‰æ•ˆå¸§æ•°"""
        valid_counts = []
        
        # è¾¹ç•Œæƒ…å†µï¼šå½“è¾“å…¥å›¾åƒæ•°é‡ <= batch_size æ—¶çš„ç‰¹æ®Šå¤„ç†
        if total_images is not None and len(start_indices) <= 1:
            if len(start_indices) == 0:
                # drop_incomplete æ¨¡å¼è¿”å›ç©ºåˆ—è¡¨
                return []
            else:
                # keep_remaining, backtrack_last, fill_color æ¨¡å¼ï¼švalid_count éƒ½æ˜¯å®é™…å›¾åƒæ•°é‡
                return [total_images]
        
        for i, (start_idx, batch_count) in enumerate(zip(start_indices, batch_counts)):
            # ç»Ÿä¸€çš„valid_countè®¡ç®—é€»è¾‘ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å¼
            if i == len(start_indices) - 1:
                # æœ€åä¸€æ‰¹ï¼šå¯¹äºå•æ‰¹æ¬¡æƒ…å†µï¼Œä½¿ç”¨å®é™…å›¾åƒæ•°é‡
                if (len(start_indices) == 1 and total_images is not None and 
                    last_batch_mode != "drop_incomplete"):
                    # drop_incompleteæ¨¡å¼ä¸‹ï¼Œä¿ç•™çš„æ‰¹æ¬¡éƒ½æ˜¯å®Œæ•´çš„ï¼Œä½¿ç”¨batch_count
                    actual_images_in_batch = total_images - start_idx
                    valid_counts.append(actual_images_in_batch)
                elif last_batch_mode == "fill_color" and total_images is not None:
                    # fill_coloræ¨¡å¼æœ€åä¸€æ‰¹ï¼šè®¡ç®—å®é™…çš„åŸå§‹å›¾åƒæ•°é‡
                    remaining_images = total_images - start_idx
                    actual_images_in_batch = min(remaining_images, batch_count)
                    valid_counts.append(actual_images_in_batch)
                else:
                    # å¤šæ‰¹æ¬¡æƒ…å†µæˆ–drop_incompleteæ¨¡å¼ï¼šå…¨éƒ¨æœ‰æ•ˆ
                    valid_counts.append(batch_count)
            else:
                # éæœ€åä¸€æ‰¹ï¼šæœ‰æ•ˆæ•°é‡ = ä¸‹ä¸€æ‰¹çš„èµ·å§‹ä½ç½® - å½“å‰æ‰¹çš„èµ·å§‹ä½ç½®
                # è¿™ä¸ªé€»è¾‘é€‚ç”¨äºæ‰€æœ‰æ¨¡å¼ï¼ŒåŒ…æ‹¬fill_color
                next_start = start_indices[i + 1]
                valid_count = next_start - start_idx
                valid_counts.append(valid_count)
        
        return valid_counts
    
    def split_batch_sequential(self, image, batch_size, overlap, last_batch_mode, color="1.0"):
        """
        é¡ºåºåˆ†å‰²æ‰¹é‡å›¾ç‰‡
        """
        # éªŒè¯å‚æ•°
        total_images = len(image)
        self._validate_parameters(total_images, batch_size, overlap, last_batch_mode)
        
        # ä¿å­˜åŸå§‹å›¾åƒæ•°é‡
        original_total = total_images
        
        # å¦‚æœè¾“å…¥å›¾ç‰‡æ•°é‡å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œæ·»åŠ æŒ‡å®šé¢œè‰²å›¾è¡¥å……
        if total_images < batch_size:
            colored_images = []
            for _ in range(batch_size - total_images):
                colored_img = self._create_white_image(image, color)
                colored_images.append(colored_img)
            # å°†å½©è‰²å›¾åˆ—è¡¨åˆå¹¶ä¸ºä¸€ä¸ªtensor
            colored_batch = torch.cat(colored_images, dim=0)
            image = torch.cat([image, colored_batch], dim=0)
            total_images = len(image)
        
        # ä½¿ç”¨æ–°çš„ç»Ÿä¸€è®¡ç®—æ–¹æ³•
        start_indices = self._calculate_start_indices(total_images, batch_size, overlap, last_batch_mode)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ‰¹æ¬¡ï¼ˆdrop_incompleteæ¨¡å¼ä¸‹å›¾åƒæ•°é‡ä¸è¶³ï¼‰ï¼Œè¿”å›ç©ºç»“æœ
        if not start_indices:
            return (image[:original_total], 0, [], [], [])
        
        batch_counts = self._calculate_batch_counts(start_indices, original_total, batch_size, last_batch_mode)
        
        # å¤„ç† fill_color æ¨¡å¼çš„é¢å¤–å½©è‰²å›¾è¡¥å……
        if last_batch_mode == "fill_color":
            max_needed = max(start_idx + batch_count for start_idx, batch_count in zip(start_indices, batch_counts))
            if max_needed > total_images:
                colored_images = []
                for _ in range(max_needed - total_images):
                    colored_img = self._create_white_image(image, color)
                    colored_images.append(colored_img)
                # å°†å½©è‰²å›¾åˆ—è¡¨åˆå¹¶ä¸ºä¸€ä¸ªtensor
                colored_batch = torch.cat(colored_images, dim=0)
                image = torch.cat([image, colored_batch], dim=0)
        
        # è®¡ç®—æœ‰æ•ˆå¸§æ•°
        valid_counts = self._calculate_valid_counts(start_indices, batch_counts, overlap, last_batch_mode, original_total)
        
        # ä¿®æ­£ fill_color æ¨¡å¼ä¸‹æœ€åä¸€æ‰¹çš„æœ‰æ•ˆå¸§æ•°
        if last_batch_mode == "fill_color" and len(valid_counts) > 0:
            last_start = start_indices[-1]
            actual_remaining = original_total - last_start
            if actual_remaining > 0:
                valid_counts[-1] = actual_remaining
        
        # ç¡®å®šè¾“å‡ºå›¾åƒï¼šåªæœ‰fill_coloræ¨¡å¼éœ€è¦è¾“å‡ºåŒ…å«å½©è‰²å›¾çš„å›¾åƒï¼Œå…¶ä»–æ¨¡å¼ç›´æ¥è¾“å‡ºåŸå§‹å›¾åƒ
        if last_batch_mode == "fill_color":
            output_image = image  # å·²ç»åŒ…å«äº†å½©è‰²å›¾å¡«å……
        else:
            # æ¢å¤åˆ°åŸå§‹è¾“å…¥å›¾åƒï¼ˆå»é™¤å¯èƒ½æ·»åŠ çš„å½©è‰²å›¾å¡«å……ï¼‰
            output_image = image[:original_total]
        
        return (output_image, len(start_indices), start_indices, batch_counts, valid_counts)


class ImageListAppend:
    """
    å›¾ç‰‡åˆ—è¡¨è¿½åŠ èŠ‚ç‚¹ - å°†å›¾ç‰‡æ”¶é›†ä¸ºåˆ—è¡¨æ ¼å¼
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image_1": ("IMAGE",),
                "image_2": ("IMAGE",)
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_list",)
    FUNCTION = "image_list_append"
    CATEGORY = "1hewNodes/batch"
    
    def image_list_append(self, image_1, image_2):
        """
        å°†ä¸¤ä¸ªå›¾ç‰‡è¾“å…¥è¿½åŠ ä¸ºåˆ—è¡¨
        """
        try:
            # å¤„ç†Noneå€¼
            if image_1 is None and image_2 is None:
                return ([],)
            elif image_1 is None:
                return ([image_2],)
            elif image_2 is None:
                return ([image_1],)
            
            return self._append_to_list(image_1, image_2)
                
        except Exception as e:
            print(f"å›¾ç‰‡åˆ—è¡¨è¿½åŠ é”™è¯¯: {str(e)}")
            return ([image_1],)
    
    def _append_to_list(self, image_1, image_2):
        """
        å°†è¾“å…¥è¿½åŠ ä¸ºåˆ—è¡¨ï¼Œä¿æŒæ‰¹é‡ç»“æ„
        """
        result = []
        
        # å¤„ç†ç¬¬ä¸€ä¸ªè¾“å…¥
        if isinstance(image_1, list):
            result.extend(image_1)
        else:
            result.append(image_1)
        
        # å¤„ç†ç¬¬äºŒä¸ªè¾“å…¥
        if isinstance(image_2, list):
            result.extend(image_2)
        else:
            result.append(image_2)
        
        print(f"å›¾ç‰‡åˆ—è¡¨è¿½åŠ å®Œæˆ: æ”¶é›†äº†{len(result)}ä¸ªå›¾ç‰‡é¡¹ç›®")
        return (result,)


class MaskBatchMathOps:
    """
    è’™ç‰ˆæ‰¹é‡æ•°å­¦è¿ç®—èŠ‚ç‚¹ - æ”¯æŒæ‰¹é‡å¤„ç†æ‰€æœ‰å›¾å±‚çš„ORå’ŒANDåŠŸèƒ½
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask": ("MASK",),
                "operation": (["or", "and"], {"default": "or"})
            }
        }

    RETURN_TYPES = ("MASK",)
    RETURN_NAMES = ("mask",)
    FUNCTION = "batch_mask_math_ops"
    CATEGORY = "1hewNodes/batch"

    def batch_mask_math_ops(self, mask, operation):
        # è·å–æ‰¹æ¬¡å¤§å°
        batch_size = mask.shape[0]
        
        # å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œç›´æ¥è¿”å›
        if batch_size <= 1:
            return (mask,)
        
        # åˆ›å»ºè¾“å‡ºè’™ç‰ˆ
        output_mask = None
        
        # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç†
        for b in range(batch_size):
            current_mask = mask[b]
            
            # å°†è’™ç‰ˆè½¬æ¢ä¸ºnumpyæ•°ç»„
            if mask.is_cuda:
                mask_np = current_mask.cpu().numpy()
            else:
                mask_np = current_mask.numpy()
            
            # åˆå§‹åŒ–è¾“å‡ºè’™ç‰ˆï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè’™ç‰ˆï¼‰
            if output_mask is None:
                output_mask = mask_np.copy()
                continue
            
            # åº”ç”¨é€‰å®šçš„æ“ä½œ
            if operation == "or":
                # oræ“ä½œï¼ˆå–æœ€å¤§å€¼ï¼‰
                output_mask = np.maximum(output_mask, mask_np)
            elif operation == "and":
                # andæ“ä½œï¼ˆå–æœ€å°å€¼ï¼‰
                output_mask = np.minimum(output_mask, mask_np)
        
        # è½¬æ¢å›tensor
        output_tensor = torch.from_numpy(output_mask).unsqueeze(0)
        
        return (output_tensor,)


class MaskBatchSplit:
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask": ("MASK",),
                "take_count": ("INT", {"default": 8, "min": 1, "max": 1024, "step": 1}),
                "from_start": ("BOOLEAN", {"default": False}),
            }
        }
    
    RETURN_TYPES = ("MASK", "MASK")
    RETURN_NAMES = ("mask_1", "mask_2")
    FUNCTION = "split_batch"
    CATEGORY = "1hewNodes/batch"
    
    def split_batch(self, mask, take_count, from_start=False):
        try:
            # è·å–æ‰¹æ¬¡å¤§å°
            batch_size = mask.shape[0]
            print(f"[MaskBatchSplit] è¾“å…¥é®ç½©æ‰¹æ¬¡ä¿¡æ¯: å½¢çŠ¶={mask.shape}, æ•°æ®ç±»å‹={mask.dtype}, è®¾å¤‡={mask.device}")
            print(f"[MaskBatchSplit] æ‹†åˆ†å‚æ•°: æ€»é®ç½©æ•°={batch_size}, å–æ•°={take_count}, ä»å¼€å¤´åˆ‡={from_start}")
            
            # éªŒè¯æ‹†åˆ†æ•°é‡
            if take_count >= batch_size:
                print(f"[MaskBatchSplit] è¾¹ç•Œæƒ…å†µ: å–æ•°({take_count})å¤§äºç­‰äºæ€»é®ç½©æ•°({batch_size})")
                
                if from_start:
                    # ä»å¼€å¤´åˆ‡ï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯å…¨éƒ¨é®ç½©ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸ºç©º
                    print(f"[MaskBatchSplit] from_start=True: ç¬¬ä¸€éƒ¨åˆ†=å…¨éƒ¨é®ç½©ï¼Œç¬¬äºŒéƒ¨åˆ†=ç©º")
                    empty_second = torch.empty((0,) + mask.shape[1:], dtype=mask.dtype, device=mask.device)
                    print(f"[MaskBatchSplit] è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=åŸé®ç½©({batch_size}ä¸ª), ç¬¬äºŒéƒ¨åˆ†=ç©ºå¼ é‡")
                    return (mask, empty_second)
                else:
                    # ä»ç»“å°¾åˆ‡ï¼šç¬¬ä¸€éƒ¨åˆ†ä¸ºç©ºï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯å…¨éƒ¨é®ç½©
                    print(f"[MaskBatchSplit] from_start=False: ç¬¬ä¸€éƒ¨åˆ†=ç©ºï¼Œç¬¬äºŒéƒ¨åˆ†=å…¨éƒ¨é®ç½©")
                    empty_first = torch.empty((0,) + mask.shape[1:], dtype=mask.dtype, device=mask.device)
                    print(f"[MaskBatchSplit] è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=ç©ºå¼ é‡, ç¬¬äºŒéƒ¨åˆ†=åŸé®ç½©({batch_size}ä¸ª)")
                    return (empty_first, mask)
            
            # æ ¹æ®å‚æ•°è®¡ç®—æ‹†åˆ†ä½ç½®
            if from_start:
                # ä»å¼€å¤´åˆ‡ï¼štake_countæ˜¯ç¬¬ä¸€éƒ¨åˆ†çš„æ•°é‡
                first_count = take_count
                second_count = batch_size - take_count
                first_batch = mask[:first_count]
                second_batch = mask[first_count:]
                print(f"[MaskBatchSplit] from_start=Trueæ‹†åˆ†å®Œæˆ: æ€»æ•°{batch_size} -> ç¬¬ä¸€éƒ¨åˆ†{first_count}ä¸ª, ç¬¬äºŒéƒ¨åˆ†{second_count}ä¸ª")
            else:
                # ä»ç»“å°¾åˆ‡ï¼štake_countæ˜¯ç¬¬äºŒéƒ¨åˆ†çš„æ•°é‡
                first_count = batch_size - take_count
                second_count = take_count
                first_batch = mask[:first_count]
                second_batch = mask[first_count:]
                print(f"[MaskBatchSplit] from_start=Falseæ‹†åˆ†å®Œæˆ: æ€»æ•°{batch_size} -> ç¬¬ä¸€éƒ¨åˆ†{first_count}ä¸ª, ç¬¬äºŒéƒ¨åˆ†{second_count}ä¸ª")
            
            print(f"[MaskBatchSplit] è¾“å‡ºå½¢çŠ¶: ç¬¬ä¸€éƒ¨åˆ†={first_batch.shape}, ç¬¬äºŒéƒ¨åˆ†={second_batch.shape}")
            return (first_batch, second_batch)
            
        except Exception as e:
            print(f"[MaskBatchSplit] é”™è¯¯: {str(e)}")
            print(f"[MaskBatchSplit] å¼‚å¸¸å¤„ç†: è¿”å›åŸé®ç½©å’Œç©ºå¼ é‡")
            # å‡ºé”™æ—¶è¿”å›åŸé®ç½©å’Œç©ºå¼ é‡
            empty_batch = torch.empty((0,) + mask.shape[1:], dtype=mask.dtype, device=mask.device)
            print(f"[MaskBatchSplit] å¼‚å¸¸è¾“å‡º: ç¬¬ä¸€éƒ¨åˆ†=åŸé®ç½©, ç¬¬äºŒéƒ¨åˆ†=ç©ºå¼ é‡")
            return (mask, empty_batch)


class VideoCutGroup:
    """
    VideoCutGroup - è§†é¢‘ç¡¬åˆ‡æ£€æµ‹èŠ‚ç‚¹
    
    è¿™æ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹è§†é¢‘ä¸­åœºæ™¯åˆ‡æ¢çš„èŠ‚ç‚¹ï¼Œé€šè¿‡åˆ†æç›¸é‚»å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥è¯†åˆ«ç¡¬åˆ‡ç‚¹ã€‚
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - æ”¯æŒä¸¤ç§æ£€æµ‹æ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å¼å’Œç²¾ç¡®æ¨¡å¼
    - å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–çš„SSIMè®¡ç®—ï¼Œé€‚åˆå¿«é€Ÿé¢„è§ˆ
    - ç²¾ç¡®æ¨¡å¼ï¼šä½¿ç”¨å¤šæ ¸æ¨¡ç³ŠSSIMè®¡ç®—ï¼Œæä¾›æ›´å‡†ç¡®çš„æ£€æµ‹ç»“æœ
    - çµæ´»çš„é˜ˆå€¼é…ç½®ï¼šæ”¯æŒå•ä¸€é˜ˆå€¼æˆ–å¤šé˜ˆå€¼æ£€æµ‹
    - æ™ºèƒ½åˆ†ç»„ï¼šæ ¹æ®æœ€å°/æœ€å¤§å¸§æ•°è¦æ±‚è‡ªåŠ¨è°ƒæ•´åˆ†ç»„
    - æ‰‹åŠ¨è°ƒæ•´ï¼šæ”¯æŒæ‰‹åŠ¨æ·»åŠ æˆ–åˆ é™¤ç‰¹å®šçš„åˆ‡ç‚¹
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "threshold_base": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.01}),
                "threshold_range": ("FLOAT", {"default": 0.05, "min": 0.01, "max": 0.2, "step": 0.01}),
                "threshold_count": ("INT", {"default": 2, "min": 1, "max": 10, "step": 1}),
                "kernel": ("STRING", {"default": "3, 7, 11", "multiline": False}),
                "min_frame_count": ("INT", {"default": 10, "min": 1, "max": 1000, "step": 1}),
                "max_frame_count": ("INT", {"default": 0, "min": 0, "max": 10000, "step": 1}),
                "fast": ("BOOLEAN", {"default": False}),

                "add_frame": ("STRING", {"default": ""}),
                "delete_frame": ("STRING", {"default": ""}),
            }
        }

    RETURN_TYPES = ("IMAGE", "INT", "INT", "INT")
    RETURN_NAMES = ("image", "group_total", "start_index", "batch_count")
    OUTPUT_IS_LIST = (False, False, True, True)
    FUNCTION = "execute"
    CATEGORY = "1hewNodes/batch"

    def __init__(self):
        # åŠ¨æ€çš„æ ¸é…ç½®ï¼šå°†åœ¨executeæ–¹æ³•ä¸­æ ¹æ®ç”¨æˆ·è¾“å…¥è®¾ç½®
        self.kernel_configs = None
        
        # å›ºå®šå‚æ•°
        self.enable_blur = True
        self.enable_kernel = True
        self.vote_ratio = 1.0  # å…¨éƒ¨ä¿ç•™
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'detection_time': 0,
            'total_frames': 0,
            'keyframes_found': 0
        }
    
    def parse_user_frames(self, frame_string):
        """
        è§£æç”¨æˆ·è¾“å…¥çš„å¸§ç´¢å¼•å­—ç¬¦ä¸²ï¼Œæ”¯æŒé€—å·åˆ†éš”ï¼Œæ™ºèƒ½å¤„ç†ä¸­è‹±æ–‡é€—å·å’Œç©ºæ ¼
        """
        if not frame_string or not frame_string.strip():
            return []
        
        # æ›¿æ¢ä¸­æ–‡é€—å·ä¸ºè‹±æ–‡é€—å·ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'[ï¼Œ,]\s*', ',', frame_string.strip())
        
        # åˆ†å‰²å¹¶è½¬æ¢ä¸ºæ•´æ•°
        frame_indices = []
        for item in normalized.split(','):
            item = item.strip()
            if item and item.isdigit():
                frame_indices.append(int(item))
        
        return sorted(list(set(frame_indices)))  # å»é‡å¹¶æ’åº

    def parse_custom_kernels(self, kernel_string):
        """
        è§£æç”¨æˆ·è¾“å…¥çš„kernelé…ç½®å­—ç¬¦ä¸²ï¼Œæ”¯æŒé€—å·åˆ†éš”
        æ ¼å¼: "3,7,11" æˆ– "3,5,7,9,11,13,15"
        è¿”å›: [(kernel_size, sigma), ...] æ ¼å¼çš„åˆ—è¡¨
        """
        if not kernel_string or not kernel_string.strip():
            # å¦‚æœä¸ºç©ºï¼Œè¿”å›é»˜è®¤é…ç½®
            return [(3, 0.6), (7, 1.0), (11, 1.5)]
        
        # æ›¿æ¢ä¸­æ–‡é€—å·ä¸ºè‹±æ–‡é€—å·ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'[ï¼Œ,]\s*', ',', kernel_string.strip())
        
        # åˆ†å‰²å¹¶è½¬æ¢ä¸ºæ•´æ•°
        kernel_sizes = []
        for item in normalized.split(','):
            item = item.strip()
            if item and item.isdigit():
                size = int(item)
                # éªŒè¯kernelå¤§å°å¿…é¡»æ˜¯å¥‡æ•°ä¸”å¤§äºç­‰äº3
                if size >= 3 and size % 2 == 1:
                    kernel_sizes.append(size)
        
        # å»é‡å¹¶æ’åº
        kernel_sizes = sorted(list(set(kernel_sizes)))
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„kernelï¼Œè¿”å›é»˜è®¤é…ç½®
        if not kernel_sizes:
            return [(3, 0.6), (7, 1.0), (11, 1.5)]
        
        # ä¸ºæ¯ä¸ªkernelå¤§å°ç”Ÿæˆå¯¹åº”çš„sigmaå€¼
        # sigma = kernel_size * 0.2 (ç»éªŒå…¬å¼)
        kernel_configs = []
        for size in kernel_sizes:
            sigma = size * 0.2
            kernel_configs.append((size, sigma))
        
        return kernel_configs

    def simple_ssim(self, img1, img2, C1=0.01**2, C2=0.03**2):
        """
        ç®€æ˜“SSIMè®¡ç®—æ–¹æ³•ï¼Œç”¨äºå¿«é€Ÿæ¨¡å¼
        """
        # å¦‚æœæ˜¯å½©è‰²å›¾åƒï¼Œå…ˆè½¬ä¸ºç°åº¦
        if img1.shape[-1] == 3:
            img1 = 0.299 * img1[..., 0] + 0.587 * img1[..., 1] + 0.114 * img1[..., 2]
            img2 = 0.299 * img2[..., 0] + 0.587 * img2[..., 1] + 0.114 * img2[..., 2]
        
        # è®¡ç®—å‡å€¼
        mu1 = img1.mean()
        mu2 = img2.mean()
        
        # è®¡ç®—æ–¹å·®
        sigma1 = ((img1 - mu1) ** 2).mean()
        sigma2 = ((img2 - mu2) ** 2).mean()
        
        # è®¡ç®—åæ–¹å·®
        sigma12 = ((img1 - mu1) * (img2 - mu2)).mean()
        
        # è®¡ç®—SSIM
        ssim = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1 + sigma2 + C2))
        
        # ä¿è¯ç»“æœåœ¨0~1ä¹‹é—´
        return max(0.0, min(1.0, ssim.item() if hasattr(ssim, 'item') else float(ssim)))

    def preprocess_images_batch(self, images_np):
        """
        æ‰¹é‡é¢„å¤„ç†æ‰€æœ‰å›¾åƒï¼Œé¿å…é‡å¤è½¬æ¢
        """
        # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
        if images_np.dtype != np.float32:
            images_np = images_np.astype(np.float32)
        
        # å¦‚æœå›¾åƒå€¼åœ¨ [0, 1] èŒƒå›´å†…ï¼Œè½¬æ¢ä¸º [0, 255] ä»¥ä¾¿SSIMè®¡ç®—
        if images_np.max() <= 1.0:
            images_np = images_np * 255.0
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒä»¥æé«˜æ£€æµ‹æ•ˆæœ
        if len(images_np.shape) == 4 and images_np.shape[3] == 3:
            # RGBè½¬ç°åº¦ï¼š0.299*R + 0.587*G + 0.114*B
            images_np = np.dot(images_np[..., :3], [0.299, 0.587, 0.114])
        elif len(images_np.shape) == 4 and images_np.shape[3] == 1:
            # å·²ç»æ˜¯å•é€šé“ï¼Œå»æ‰æœ€åä¸€ä¸ªç»´åº¦
            images_np = images_np.squeeze(-1)
        
        return images_np

    def batch_calculate_ssim_matrix(self, processed_images):
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰ç›¸é‚»å¸§çš„SSIMå€¼çŸ©é˜µï¼Œä½¿ç”¨å›ºå®šçš„æ ¸é…ç½®å’Œæ¨¡ç³Šæ¨¡å¼
        """
        B = processed_images.shape[0]
        if B <= 1:
            return {}
        
        ssim_matrix = {}
        
        # ä¸ºæ¯ä¸ªæ ¸é…ç½®è®¡ç®—æ¨¡ç³ŠSSIM
        for kernel_idx, kernel_config in enumerate(self.kernel_configs):
            kernel_size, sigma = kernel_config
            ssim_values = np.zeros(B - 1, dtype=np.float32)
            
            # è®¡ç®—æ‰€æœ‰ç›¸é‚»å¸§çš„æ¨¡ç³ŠSSIM
            for i in range(B - 1):
                ssim_val = self._blur_pixel_ssim(
                    processed_images[i], processed_images[i + 1], kernel_size, sigma
                )
                ssim_values[i] = ssim_val
            
            ssim_matrix[kernel_idx] = ssim_values
        
        return ssim_matrix

    def _blur_pixel_ssim(self, img1, img2, kernel_size, sigma):
        """æ¨¡ç³Šåƒç´ SSIMè®¡ç®—ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
        ksize = (kernel_size, kernel_size)
        img1_blur = cv2.GaussianBlur(img1, ksize, sigma)
        img2_blur = cv2.GaussianBlur(img2, ksize, sigma)
        
        # è®¡ç®—å‡å€¼
        mu1 = cv2.boxFilter(img1_blur, -1, (kernel_size, kernel_size))
        mu2 = cv2.boxFilter(img2_blur, -1, (kernel_size, kernel_size))
        
        mu1_sq = mu1 * mu1
        mu2_sq = mu2 * mu2
        mu1_mu2 = mu1 * mu2
        
        # è®¡ç®—æ–¹å·®å’Œåæ–¹å·®
        sigma1_sq = cv2.boxFilter(img1_blur * img1_blur, -1, (kernel_size, kernel_size)) - mu1_sq
        sigma2_sq = cv2.boxFilter(img2_blur * img2_blur, -1, (kernel_size, kernel_size)) - mu2_sq
        sigma12 = cv2.boxFilter(img1_blur * img2_blur, -1, (kernel_size, kernel_size)) - mu1_mu2
        
        # SSIMå¸¸æ•°
        C1 = (0.01) ** 2
        C2 = (0.03) ** 2
        
        # è®¡ç®—SSIM
        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        
        return float(np.mean(ssim_map))

    def generate_dynamic_thresholds(self, threshold_base, threshold_range=0.05, threshold_count=2):
        """
        ç”ŸæˆåŠ¨æ€æ•°é‡çš„é˜ˆå€¼
        threshold_count=1: åªä½¿ç”¨threshold_base
        threshold_count=2: èŒƒå›´ä¸¤ç«¯ [base-range, base+range]
        threshold_count=3: ä¸¤ç«¯+ä¸­é—´ [base-range, base, base+range]
        threshold_count=4+: åœ¨èŒƒå›´å†…å‡åŒ€åˆ†å¸ƒ
        """
        if threshold_count == 1:
            # åªä½¿ç”¨åŸºç¡€é˜ˆå€¼
            thresholds = [threshold_base]
        elif threshold_count == 2:
            # èŒƒå›´ä¸¤ç«¯
            thresholds = [
                threshold_base - threshold_range,  # ä¸‹é™
                threshold_base + threshold_range   # ä¸Šé™
            ]
        else:
            # 3ä¸ªæˆ–æ›´å¤šï¼šåœ¨èŒƒå›´å†…å‡åŒ€åˆ†å¸ƒ
            min_threshold = threshold_base - threshold_range
            max_threshold = threshold_base + threshold_range
            
            if threshold_count == 3:
                # ç‰¹æ®Šå¤„ç†ï¼šä¸¤ç«¯+ä¸­é—´
                thresholds = [min_threshold, threshold_base, max_threshold]
            else:
                # 4ä¸ªæˆ–æ›´å¤šï¼šå‡åŒ€åˆ†å¸ƒ
                step = (max_threshold - min_threshold) / (threshold_count - 1)
                thresholds = [min_threshold + i * step for i in range(threshold_count)]
        
        # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
        thresholds = [max(0.0, min(1.0, t)) for t in thresholds]
        
        return sorted(thresholds)

    def optimized_single_threshold_detection(self, ssim_matrix, user_threshold, kernel_idx, 
                                           min_frame_count, max_frame_count, total_frames):
        """
        åŸºäºé¢„è®¡ç®—SSIMçŸ©é˜µçš„çº¯ç²¹é˜ˆå€¼æ£€æµ‹æ–¹æ³•
        ä¿®æ­£é˜ˆå€¼é€»è¾‘ï¼šç”¨æˆ·é˜ˆå€¼è¶Šå¤§ï¼Œæ£€æµ‹è¶Šä¸¥æ ¼ï¼Œç”»é¢è¶Šå°‘
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•åªè¿›è¡Œçº¯ç²¹çš„é˜ˆå€¼æ£€æµ‹ï¼Œä¸åº”ç”¨åˆ†ç»„è§„åˆ™
        åˆ†ç»„è§„åˆ™åº”è¯¥åœ¨æ‰€æœ‰é˜ˆå€¼æ£€æµ‹å®Œæˆåï¼Œåœ¨æœ€ç»ˆåˆå¹¶é˜¶æ®µç»Ÿä¸€åº”ç”¨
        """
        B = total_frames
        if B <= 1:
            return [0]
        
        # è·å–å¯¹åº”çš„SSIMå€¼æ•°ç»„
        if kernel_idx not in ssim_matrix:
            return [0]
        
        ssim_values = ssim_matrix[kernel_idx]
        
        # çº¯ç²¹çš„é˜ˆå€¼æ£€æµ‹ï¼šæ‰¾å‡ºæ‰€æœ‰æ»¡è¶³é˜ˆå€¼æ¡ä»¶çš„ç¡¬åˆ‡ç‚¹
        threshold_cuts = [0]  # ç¬¬ä¸€ç»„æ€»æ˜¯ä»0å¼€å§‹
        for i in range(len(ssim_values)):
            ssim_val = ssim_values[i]
            # ç›´æ¥æ¯”è¾ƒï¼š(1-ssim)å¤§äºç”¨æˆ·é˜ˆå€¼æ—¶ï¼Œè®¤ä¸ºæ˜¯ç¡¬åˆ‡
            if (1.0 - ssim_val) > user_threshold:
                cut_point = i + 1  # ç¡¬åˆ‡åçš„å¸§ä½œä¸ºæ–°ç»„çš„èµ·å§‹
                if cut_point < B and cut_point not in threshold_cuts:
                    threshold_cuts.append(cut_point)
        
        return threshold_cuts


    def batch_detection_all_features(self, images_np, threshold_base, min_frame_count, max_frame_count, threshold_range=0.05, threshold_count=2):
        """
        æ‰¹é‡æ£€æµ‹æ‰€æœ‰ç‰¹å¾ç»„åˆï¼Œä½¿ç”¨å›ºå®šçš„æ ¸é…ç½®å’Œå‚æ•°
        """
        total_frames = len(images_np)
        
        if total_frames < 2:
            return [[0]]
        
        # é¢„å¤„ç†å›¾åƒ
        processed_images = self.preprocess_images_batch(images_np)
        
        # æ‰¹é‡è®¡ç®—SSIMçŸ©é˜µ
        ssim_matrix = self.batch_calculate_ssim_matrix(processed_images)
        
        # ç”ŸæˆåŠ¨æ€æ•°é‡çš„é˜ˆå€¼
        user_thresholds = self.generate_dynamic_thresholds(threshold_base, threshold_range, threshold_count)
        
        # æ‰“å°æ£€æµ‹ä»»åŠ¡æ¦‚è§ˆ
        print()
        print("=== ğŸš€ VideoCutGroup å¤šæ ¸æ¨¡ç³Šæ¨¡å¼æ£€æµ‹ å¯åŠ¨ ===")
        print(f"threshold: {[f'{t:.3f}' for t in user_thresholds]}")
        kernel_list = [str(k[0]) for k in self.kernel_configs]
        print(f"kernel: [{','.join(kernel_list)}]")
        print()
        
        total_groups = len(self.kernel_configs) * len(user_thresholds)
        print(f"ğŸ“ˆ {total_groups} ç»„æ£€æµ‹ä»»åŠ¡è¯¦æƒ…")
        
        # å¯¹æ¯ä¸ªæ ¸å’Œæ¯ä¸ªé˜ˆå€¼è¿›è¡Œæ£€æµ‹
        all_detection_results = []
        group_num = 1
        
        for kernel_idx in range(len(self.kernel_configs)):
            kernel_size, sigma = self.kernel_configs[kernel_idx]
            
            for user_threshold in user_thresholds:
                # ä½¿ç”¨ä¼˜åŒ–çš„æ£€æµ‹æ–¹æ³•
                result = self.optimized_single_threshold_detection(
                    ssim_matrix, user_threshold, kernel_idx, min_frame_count, max_frame_count, total_frames
                )
                all_detection_results.append(result)
                
                # è·å–é˜ˆå€¼è¯¦ç»†ä¿¡æ¯ç”¨äºæ—¥å¿— - åŸºäºå®é™…æ£€æµ‹ç»“æœ
                threshold_details = []
                if kernel_idx in ssim_matrix and len(result) > 1:
                    ssim_values = ssim_matrix[kernel_idx]
                    # éå†å®é™…æ£€æµ‹åˆ°çš„åˆ‡ç‚¹ï¼ˆæ’é™¤èµ·å§‹ç‚¹0ï¼‰
                    for cut_point in result[1:]:  # è·³è¿‡èµ·å§‹ç‚¹0
                        if cut_point > 0 and cut_point <= len(ssim_values):
                            # åˆ‡ç‚¹å¯¹åº”çš„æ˜¯å‰ä¸€å¸§ä¸å½“å‰å¸§çš„æ¯”è¾ƒ
                            ssim_val = ssim_values[cut_point - 1]
                            threshold_val = 1.0 - ssim_val
                            threshold_details.append(f"{cut_point}:{threshold_val:.3f}")
                
                # æ ¼å¼åŒ–æ—¥å¿—è¾“å‡ºï¼ˆæ’é™¤èµ·å§‹ç‚¹0ï¼‰
                actual_cut_points = len(result) - 1 if result and result[0] == 0 else len(result)
                print(f"ğŸ” ç¬¬{group_num}ç»„ï¼š threshold={user_threshold:.3f}ï¼Œkernel = {kernel_size}")
                print(f"- æ£€æµ‹åˆ‡ç‚¹ï¼š{actual_cut_points} ä¸ª [indexï¼šthreshold]")
                if threshold_details:
                    print(f"- [{', '.join(threshold_details)}]")
                print()
                
                group_num += 1
        
        # å­˜å‚¨æ£€æµ‹ç»“æœç”¨äºåç»­æ±‡æ€»
        self._detection_results_summary = {
            'user_thresholds': user_thresholds,
            'kernel_configs': self.kernel_configs,
            'all_detection_results': all_detection_results
        }
        
        return all_detection_results

    def unified_voting_fusion(self, all_detection_results, total_frames, min_frame_count=10, max_frame_count=0):
        """
        ç»Ÿä¸€èåˆæ–¹æ³•ï¼Œæ•´åˆæ‰€æœ‰æ£€æµ‹ç»“æœå¹¶åº”ç”¨åˆ†ç»„è§„åˆ™
        
        é‡è¦ï¼šåœ¨æ­¤é˜¶æ®µç»Ÿä¸€åº”ç”¨min_frame_countå’Œmax_frame_countè§„åˆ™ï¼Œ
        ç¡®ä¿æ‰€æœ‰é˜ˆå€¼ç»„åˆéƒ½ä½¿ç”¨ç›¸åŒçš„åˆ†ç»„ç­–ç•¥
        """
        if not all_detection_results:
            return [0]
        
        # æ‰“å°æœ€ç»ˆæ£€æµ‹ç»“æœæ±‡æ€»
        print()
        print("âœ… æœ€ç»ˆæ£€æµ‹ç»“æœ")
        if hasattr(self, '_detection_results_summary'):
            summary = self._detection_results_summary
            user_thresholds = summary['user_thresholds']
            kernel_configs = summary['kernel_configs']
            results = summary['all_detection_results']
            
            # æŒ‰ç»„æ˜¾ç¤ºæ¯ä¸ªæ£€æµ‹ç»“æœ
            result_idx = 0
            for kernel_idx, (kernel_size, sigma) in enumerate(kernel_configs):
                for threshold in user_thresholds:
                    if result_idx < len(results):
                        start_indices = results[result_idx]  # åŒ…å«èµ·å§‹ç‚¹0çš„å®Œæ•´åˆ—è¡¨
                        cut_points_count = len(start_indices)
                        print(f"[threshold={threshold:.3f}ï¼Œkernel = {kernel_size}]: start index å…±è®¡ {cut_points_count} ä¸ª")
                        print(f"{start_indices}")
                        result_idx += 1
        
        # æ”¶é›†æ‰€æœ‰åˆ‡ç‚¹ï¼ˆå»é‡ï¼‰
        all_cut_points = set([0])  # èµ·å§‹å¸§
        for result in all_detection_results:
            for frame in result[1:]:  # è·³è¿‡èµ·å§‹å¸§0
                all_cut_points.add(frame)
        
        # æ’åº
        raw_cut_points = sorted(list(all_cut_points))
        
        # æ˜¾ç¤ºåˆå¹¶å‰çš„ç»“æœ
        print(f"åˆå¹¶æ£€æµ‹ç»“æœï¼š{len(raw_cut_points)} ä¸ª")
        print(f"{raw_cut_points}")
        print()
        
        # åº”ç”¨åˆ†ç»„è§„åˆ™
        final_cut_points = self._apply_final_grouping_rules(raw_cut_points, min_frame_count, max_frame_count, total_frames)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"min_frame_count={min_frame_count}, max_frame_count={max_frame_count} ï¼Œåˆ†ç»„è§„åˆ™å¤„ç†åï¼š{len(final_cut_points)} ä¸ª")
        print(f"{final_cut_points}")
        print()
        
        return final_cut_points
    
    def _apply_final_grouping_rules(self, cut_points, min_frame_count, max_frame_count, total_frames):
        """
        åœ¨æœ€ç»ˆé˜¶æ®µåº”ç”¨åˆ†ç»„è§„åˆ™
        """
        if not cut_points or len(cut_points) <= 1:
            return [0]
        
        # åº”ç”¨min_frame_countè§„åˆ™ï¼šåˆå¹¶è¿‡è¿‘çš„åˆ‡ç‚¹
        filtered_points = [cut_points[0]]  # ä¿ç•™èµ·å§‹ç‚¹0
        
        for point in cut_points[1:]:
            if point - filtered_points[-1] >= min_frame_count:
                filtered_points.append(point)
            # å¦‚æœè·ç¦»å¤ªè¿‘ï¼Œè·³è¿‡è¿™ä¸ªåˆ‡ç‚¹
        
        # åº”ç”¨max_frame_countè§„åˆ™ï¼šæ‹†åˆ†è¿‡é•¿çš„æ®µ
        if max_frame_count > 0:
            final_points = [0]
            
            for i in range(1, len(filtered_points)):
                start = filtered_points[i-1]
                end = filtered_points[i]
                segment_length = end - start
                
                # å¦‚æœæ®µé•¿åº¦è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦æ‹†åˆ†
                if segment_length > max_frame_count:
                    # åœ¨è¿™ä¸ªæ®µå†…æŒ‰max_frame_counté—´éš”æ’å…¥åˆ‡ç‚¹
                    current = start
                    while current + max_frame_count < end:
                        current += max_frame_count
                        final_points.append(current)
                
                # æ·»åŠ åŸå§‹åˆ‡ç‚¹
                final_points.append(end)
            
            # å¤„ç†æœ€åä¸€æ®µï¼ˆåˆ°è§†é¢‘ç»“å°¾ï¼‰
            if len(filtered_points) > 0:
                last_point = filtered_points[-1]
                if last_point < total_frames:
                    remaining_length = total_frames - last_point
                    if remaining_length > max_frame_count:
                        current = last_point
                        while current + max_frame_count < total_frames:
                            current += max_frame_count
                            final_points.append(current)
            
            return sorted(list(set(final_points)))
        else:
            return filtered_points

    def sequential_detection(self, images, threshold_base, min_frame_count, max_frame_count, threshold_range=0.05, threshold_count=2):
        """
        åºåˆ—æ£€æµ‹æ–¹æ³•ï¼Œæ ¹æ®å‚æ•°é…ç½®è¿›è¡Œè§†é¢‘ç¡¬åˆ‡æ£€æµ‹
        """
        # è½¬æ¢å›¾åƒæ ¼å¼
        if hasattr(images, 'cpu'):
            images_np = images.cpu().numpy()
        else:
            images_np = images
        
        # æ‰¹é‡æ£€æµ‹æ‰€æœ‰ç‰¹å¾ç»„åˆ
        all_detection_results = self.batch_detection_all_features(
            images_np, threshold_base, min_frame_count, max_frame_count, threshold_range, threshold_count
        )
        
        # ç»Ÿä¸€æŠ•ç¥¨èåˆ
        final_split_points = self.unified_voting_fusion(all_detection_results, len(images_np), min_frame_count, max_frame_count)
        
        return final_split_points

    def fast_mode_detection(self, images, threshold_base, min_frame_count, max_frame_count):
        """
        å¿«é€Ÿæ¨¡å¼æ£€æµ‹ï¼Œä½¿ç”¨ç®€åŒ–çš„SSIMè®¡ç®—æ–¹æ³•
        æ³¨æ„ï¼šè¿™é‡Œçš„threshold_baseå·²ç»ç»è¿‡1-å¤„ç†ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹é˜ˆå€¼é€»è¾‘
        """
        # è½¬æ¢å›¾åƒæ ¼å¼
        if hasattr(images, 'cpu'):
            images_np = images.cpu().numpy()
        else:
            images_np = images
        
        B = images_np.shape[0]
        if B < 2:
            return [0]
        
        # æ‰“å°å¿«é€Ÿæ¨¡å¼æ¦‚è§ˆ
        print()
        print("=== âš¡ VideoCutGroup fast æ¨¡å¼æ£€æµ‹ å¯åŠ¨ ===")
        print(f"threshold={threshold_base:.3f}ï¼Œ")
        
        # è®¡ç®—æ‰€æœ‰ç›¸é‚»å¸§çš„ç®€æ˜“SSIM
        ssim_list = []
        for i in range(B - 1):
            ssim_val = self.simple_ssim(images_np[i], images_np[i + 1])
            ssim_list.append(ssim_val)
        
        if not ssim_list:
            return [0]
        
        # ä½¿ç”¨ç±»ä¼¼nodes.pyçš„åŠ¨æ€é˜ˆå€¼è®¡ç®—
        ssim_max = max(ssim_list)
        ssim_mean = sum(ssim_list) / len(ssim_list)
        
        # å°†ç”¨æˆ·é˜ˆå€¼è½¬æ¢ä¸ºæ£€æµ‹é˜ˆå€¼
        # ç”±äºthreshold_baseç»è¿‡äº†1-å¤„ç†ï¼Œè¿™é‡Œéœ€è¦è½¬æ¢å›åŸå§‹é€»è¾‘
        # ç”¨æˆ·æœŸæœ›ï¼šthreshold_baseè¶Šå¤§ï¼Œæ£€æµ‹è¶Šä¸¥æ ¼ï¼Œç”»é¢è¶Šå°‘
        # è½¬æ¢ï¼šthreshold_base -> (1 - threshold_base) -> ä½œä¸ºæ£€æµ‹ç®—æ³•ä¸­çš„thresholdå‚æ•°
        nodes_threshold = 1.0 - threshold_base
        
        # ä½¿ç”¨nodes.pyçš„é˜ˆå€¼è®¡ç®—é€»è¾‘
        ssim_limit = ssim_max - (ssim_max - ssim_mean) * 2 - nodes_threshold
        
        # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†… [0, 1]
        ssim_limit = max(0.0, min(1.0, ssim_limit))
        
        # æ‰¾åˆ°æ‰€æœ‰ä½äºé˜ˆå€¼çš„åˆ‡ç‚¹
        keyframes = [0]
        threshold_details = []  # å­˜å‚¨ç´¢å¼•å’Œå¯¹åº”çš„é˜ˆå€¼å€¼(1-ssim)
        for i, ssim_val in enumerate(ssim_list):
            if ssim_val < ssim_limit:
                keyframes.append(i + 1)
                threshold_val = 1.0 - ssim_val  # è½¬æ¢ä¸ºé˜ˆå€¼å€¼
                threshold_details.append(f"{i+1}:{threshold_val:.3f}")
        
        # æ˜¾ç¤ºåˆå§‹æ£€æµ‹ç»“æœ
        print(f"æ£€æµ‹ç»“æœï¼š{len(keyframes)} ä¸ª")
        print(f"{keyframes}")
        
        # åº”ç”¨æœ€å°å¸§æ•°é™åˆ¶ï¼Œåˆå¹¶è¿‡è¿‘çš„åˆ‡ç‚¹
        filtered_keyframes = [keyframes[0]]
        for kf in keyframes[1:]:
            if kf - filtered_keyframes[-1] > min_frame_count:
                filtered_keyframes.append(kf)
            else:
                filtered_keyframes[-1] = kf  # æ›¿æ¢ä¸ºæ›´å¤§çš„ç´¢å¼•
        
        # æ£€æŸ¥å°¾éƒ¨ï¼šå¦‚æœå°¾éƒ¨åˆ°è§†é¢‘ç»“å°¾çš„å¸§æ•° < min_frame_countï¼Œå‘å·¦å½’å¹¶
        while len(filtered_keyframes) > 1 and (B - filtered_keyframes[-1]) < min_frame_count:
            filtered_keyframes.pop()
        
        # åº”ç”¨æœ€å¤§å¸§æ•°é™åˆ¶ï¼Œæ‹†åˆ†è¿‡é•¿çš„æ®µ
        if max_frame_count > 0:
            final_keyframes = [0]
            for i in range(1, len(filtered_keyframes) + 1):
                start = filtered_keyframes[i - 1]
                end = filtered_keyframes[i] if i < len(filtered_keyframes) else B
                segment_length = end - start
                
                if segment_length > max_frame_count:
                    # æ‹†åˆ†é•¿æ®µ
                    num_splits = math.ceil(segment_length / max_frame_count)
                    frames_per_split = segment_length // num_splits
                    
                    for j in range(num_splits):
                        if j < num_splits - 1:
                            final_keyframes.append(start + (j + 1) * frames_per_split)
                        else:
                            final_keyframes.append(end)
                else:
                    final_keyframes.append(end)
        else:
            final_keyframes = filtered_keyframes[:]
            if final_keyframes[-1] != B:
                final_keyframes.append(B)
        
        # ç§»é™¤æœ€åä¸€ä¸ªç‚¹ï¼ˆå¦‚æœæ˜¯Bï¼‰
        if final_keyframes and final_keyframes[-1] == B:
            final_keyframes.pop()
        
        # æ˜¾ç¤ºåˆ†ç»„è§„åˆ™å¤„ç†åçš„ç»“æœ
        max_frame_text = "0" if max_frame_count == 0 else str(max_frame_count)
        print(f"min_frame_count={min_frame_count}ï¼Œ max_frame_count={max_frame_text} ï¼Œåˆ†ç»„è§„åˆ™å¤„ç†åï¼š{len(final_keyframes)} ä¸ª")
        print(f"{final_keyframes}")
        
        return final_keyframes

    def apply_user_modifications(self, cut_points, add_frame, delete_frame, total_frames):
        """
        åº”ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„æ·»åŠ å’Œåˆ é™¤å¸§ä¿®æ”¹
        """
        # è§£æç”¨æˆ·è¾“å…¥
        add_frames = self.parse_user_frames(add_frame)
        delete_frames = self.parse_user_frames(delete_frame)
        
        # åº”ç”¨ä¿®æ”¹
        modified_cut_points = list(cut_points)
        
        # è®°å½•å®é™…æ·»åŠ å’Œåˆ é™¤çš„å¸§
        actually_added = []
        actually_deleted = []
        
        # æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„å¸§
        for frame in add_frames:
            if 0 <= frame < total_frames and frame not in modified_cut_points:
                modified_cut_points.append(frame)
                actually_added.append(frame)
        
        # åˆ é™¤ç”¨æˆ·æŒ‡å®šçš„å¸§ï¼ˆä½†ä¿ç•™èµ·å§‹å¸§0ï¼‰
        for frame in delete_frames:
            if frame in modified_cut_points and frame != 0:
                modified_cut_points.remove(frame)
                actually_deleted.append(frame)
        
        # æ‰“å°ç”¨æˆ·è®¾å®šçš„æ‰€æœ‰æ·»åŠ å’Œåˆ é™¤ä¿¡æ¯ï¼ˆä¸æ˜¯è¿‡æ»¤åçš„ï¼‰
        if add_frames:
            print(f"â• ç”¨æˆ·æ·»åŠ å¸§: {add_frames}")
        if delete_frames:
            print(f"â– ç”¨æˆ·åˆ é™¤å¸§: {delete_frames}")
        
        # æ’åºå¹¶è¿”å›
        final_cut_points = sorted(list(set(modified_cut_points)))
        
        if add_frames or delete_frames:
            print(f"ğŸ”§ åæœŸå¢å‡å¸§å¤„ç†åï¼š{len(final_cut_points)} ä¸ª")
            print(f"{final_cut_points}")
        
        return final_cut_points

    def execute(self, image, threshold_base, threshold_range, threshold_count, min_frame_count, max_frame_count, 
                fast, add_frame, delete_frame, kernel):
        """
        ä¸»æ‰§è¡Œå‡½æ•°ï¼Œæ”¯æŒè‡ªå®šä¹‰kernelé…ç½®å’Œå¤šç§æ£€æµ‹æ¨¡å¼
        """
        try:
            # è®¾ç½®åŠ¨æ€kernelé…ç½®
            self.kernel_configs = self.parse_custom_kernels(kernel)
            # print(f"ğŸ”§ ä½¿ç”¨kernelé…ç½®: {[k for k, s in self.kernel_configs]}")
            
            B = image.shape[0]
            if B < 2:
                return (image, 1, [0], [B])
            
            # å‚æ•°éªŒè¯ï¼šmax_frame_count=0è¡¨ç¤ºæ— é™åˆ¶
            if max_frame_count > 0 and min_frame_count >= max_frame_count:
                max_frame_count = min_frame_count + 10
            
            start_time = time.time()
            
            if fast:
                # ä½¿ç”¨å¿«é€Ÿæ¨¡å¼æ£€æµ‹
                start_indices = self.fast_mode_detection(
                    image, threshold_base, min_frame_count, max_frame_count
                )
                
                detection_time = time.time() - start_time
            else:
                # ä½¿ç”¨å›ºå®šå‚æ•°çš„å¤šç‰¹å¾æ£€æµ‹ç®—æ³•
                start_indices = self.sequential_detection(
                    image, threshold_base, min_frame_count, max_frame_count, threshold_range, threshold_count
                )
                
                detection_time = time.time() - start_time
            
            # åº”ç”¨ç”¨æˆ·è‡ªå®šä¹‰ä¿®æ”¹
            original_count = len(start_indices)
            start_indices = self.apply_user_modifications(start_indices, add_frame, delete_frame, B)
            
            # æ˜¾ç¤ºå¤„ç†æ­¥éª¤å·²ç»åœ¨apply_user_modificationsä¸­å®Œæˆï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æ‰“å°
            
            # è®¡ç®—æ¯æ®µçš„å¸§æ•°
            batch_counts = []
            for i in range(len(start_indices)):
                if i < len(start_indices) - 1:
                    # å½“å‰æ®µçš„å¸§æ•° = ä¸‹ä¸€ä¸ªèµ·å§‹ç‚¹ - å½“å‰èµ·å§‹ç‚¹
                    batch_count = start_indices[i + 1] - start_indices[i]
                else:
                    # æœ€åä¸€æ®µçš„å¸§æ•° = æ€»å¸§æ•° - å½“å‰èµ·å§‹ç‚¹
                    batch_count = B - start_indices[i]
                batch_counts.append(batch_count)
            
            # è®¡ç®—åˆ†ç»„æ€»æ•°
            group_total = len(start_indices)
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            
            # æ€»ç»“æ€§æ—¥å¿—è¾“å‡ºå·²åœ¨å„è‡ªçš„æ£€æµ‹æ–¹æ³•ä¸­å®Œæˆ
            
            # æ˜¾ç¤ºå¯¹åº”æ¯ç»„å¸§æ•°
            print(f"å¯¹åº”æ¯ç»„å¸§æ•°")
            print(f"{batch_counts}")
            print()
            
            print(f"ä»»åŠ¡æ€»è€—æ—¶ï¼š{total_time:.1f} ç§’")
            if fast:
                print("=== âš¡ VideoCutGroup fast æ¨¡å¼æ£€æµ‹ å®Œæˆ ===")
            else:
                print("=== ğŸš€ VideoCutGroup å¤šæ ¸æ¨¡ç³Šæ¨¡å¼æ£€æµ‹ å®Œæˆ ===")
            print()
            
            return (
                image[start_indices],  # è¿”å›èµ·å§‹å¸§çš„å›¾åƒ
                group_total,
                start_indices,
                batch_counts
            )
            
        except Exception as e:
            print(f"VideoCutGroup æ‰§è¡Œé”™è¯¯: {str(e)}")
            return (image, 1, [0], [image.shape[0]])


NODE_CLASS_MAPPINGS = {
    "1hew_ImageBatchExtract": ImageBatchExtract,
    "1hew_ImageBatchSplit": ImageBatchSplit,
    "1hew_ImageBatchGroup": ImageBatchGroup,
    "1hew_ImageListAppend": ImageListAppend,
    "1hew_MaskBatchMathOps": MaskBatchMathOps,
    "1hew_MaskBatchSplit": MaskBatchSplit,
    "1hew_VideoCutGroup": VideoCutGroup,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "1hew_ImageBatchExtract": "Image Batch Extract",
    "1hew_ImageBatchSplit": "Image Batch Split",
    "1hew_ImageBatchGroup": "Image Batch Group",
    "1hew_ImageListAppend": "Image List Append",
    "1hew_MaskBatchMathOps": "Mask Batch Math Ops",
    "1hew_MaskBatchSplit": "Mask Batch Split",
    "1hew_VideoCutGroup": "Video Cut Group",
}
